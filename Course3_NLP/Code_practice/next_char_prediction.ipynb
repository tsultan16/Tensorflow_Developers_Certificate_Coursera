{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 16:12:57.562797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 16:12:57.636427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 16:12:57.637588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at the task of generator the next character for a given sequence of characters. This is almost identical to the task of next word prediction, however the vocabulary consists of distinct characters and is therefore much smaller. The dataset we will use is the collected works of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text: 1115394\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# read contents of text file as a single string\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(f\"Total number of characters in the text: {len(text)}\\n\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voicabulary length: 65\n",
      "['U', 't', 'j', 'H', 'f', 'u', 'O', 'n', 'G', 'W', 'i', 'b', 'v', '-', 'k', 'D', 'T', 'w', ',', 'P', 'x', 'F', 'o', 'A', ';', 'r', 'd', 'V', '?', ' ', 'h', 'Z', 'm', 'g', 'K', 'X', 'z', 'l', 'J', 'R', 'q', '.', 'C', 'N', '!', 'S', 'I', 'a', 'L', '&', 'p', '\\n', 'c', \"'\", 'B', 'e', ':', 'E', 'Q', '3', 's', 'y', '$', 'M', 'Y']\n"
     ]
    }
   ],
   "source": [
    "# generate vocabulary of characters (included both upper and lower case english alphabet and punctuation symbols)\n",
    "vocab = list(set(text))\n",
    "print(f\"Vocabulary length: {len(vocab)}\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will use a keras StringLookup layer to tokenize sequences of characacters into a sequence of integer character indices\n",
    "chars_to_index = tf.keras.layers.StringLookup(vocabulary=list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[48, 12, 53, 27, 56, 5, 34]]>\n"
     ]
    }
   ],
   "source": [
    "test = [\"abcdefg\"]\n",
    "\n",
    "# convert a string into a sequence of chars\n",
    "test = tf.strings.unicode_split(test, input_encoding='UTF-8')\n",
    "\n",
    "test_indices = chars_to_index(test)\n",
    "print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'abcdefg'], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# similarly, we will also create another StringLookup layer that performs the inverse operation of converting a sequence of character indices to characters\n",
    "index_to_chars = tf.keras.layers.StringLookup(vocabulary=chars_to_index.get_vocabulary(), invert=True)\n",
    "\n",
    "# also define a function for joining the list of characters into a string\n",
    "def text_from_indices(idx):\n",
    "    return tf.strings.reduce_join(index_to_chars(idx), axis=-1)\n",
    "\n",
    "print(text_from_indices(test_indices)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our next character prediction task, we will use a many-to-many RNN model so that the input and output sequences will be of the same length. To generate the input and output sequences, we can take a sequence of charcaters of length `sequence_len+1`, then assign the first `sequence_len` charcters as the input and the last `sequence_len` charcters as the output labels (i.e. the output labels are just the input sequence shifted forward by 1). E.g.\n",
    "\n",
    "`sequence = ['T', 'e', 'n', 's', 'o', 'r']`\n",
    "\n",
    "`x = ['T', 'e', 'n', 's', 'o']`\n",
    "\n",
    "`y = ['e', 'n', 's', 'o', 'r']`\n",
    "\n",
    "To prepare the dataset, we will therefore split the text into chunks of size `sequence_len+1` and create `(x,y)` input-label pair training examples from these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([22 11 26 ... 34 42 52], shape=(1115394,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# first convert the text into a sequence of character indices\n",
    "all_indices = chars_to_index(tf.strings.unicode_split(text, input_encoding='UTF-8'))\n",
    "print(all_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset hyper parameters\n",
    "SEQ_LENGTH = 100 # fixed sequence length for training input\n",
    "BATCH_SIZE = 64  # number of examples per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
      "\n",
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "\n",
      "tf.Tensor(\n",
      "[b'a' b'r' b'e' b' ' b'a' b'l' b'l' b' ' b'r' b'e' b's' b'o' b'l' b'v'\n",
      " b'e' b'd' b' ' b'r' b'a' b't' b'h' b'e' b'r' b' ' b't' b'o' b' ' b'd'\n",
      " b'i' b'e' b' ' b't' b'h' b'a' b'n' b' ' b't' b'o' b' ' b'f' b'a' b'm'\n",
      " b'i' b's' b'h' b'?' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'R' b'e' b's'\n",
      " b'o' b'l' b'v' b'e' b'd' b'.' b' ' b'r' b'e' b's' b'o' b'l' b'v' b'e'\n",
      " b'd' b'.' b'\\n' b'\\n' b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i'\n",
      " b'z' b'e' b'n' b':' b'\\n' b'F' b'i' b'r' b's' b't' b',' b' ' b'y' b'o'\n",
      " b'u' b' ' b'k'], shape=(101,), dtype=string)\n",
      "\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 17:13:41.864796: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [1115394]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "# next, we will use the tensorflow Dataset API to work with a stream of charcter indices and generate chunks \n",
    "all_indices_dataset = tf.data.Dataset.from_tensor_slices(all_indices)\n",
    "\n",
    "# create chunks\n",
    "sequences = all_indices_dataset.batch(SEQ_LENGTH+1, drop_remainder=True) # set drop remainder flag to ensure all batches are of the same size and any smaller remainder batch is discarded\n",
    "\n",
    "for seq in sequences.take(2):\n",
    "    print(index_to_chars(seq))\n",
    "    print()\n",
    "    print(text_from_indices(seq).numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets split the seqnuences into input-target pairs\n",
    "def split_seqeunce(sequence):\n",
    "    input = sequence[:-1]\n",
    "    target = sequence[1:]\n",
    "    return input, target\n",
    "\n",
    "dataset = sequences.map(split_seqeunce)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "\n",
      "input: b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "target: b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 17:21:56.406809: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [1115394]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "for input, target in dataset.take(2):\n",
    "    print(f\"input: {text_from_indices(input).numpy()}\")\n",
    "    print(f\"target: {text_from_indices(target).numpy()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we want to create batches of the chunks and also shuffle the chunks around (the shuffling is done on a fixed smaller buffer size to avoid loading the entire data into memory for the shuffling, also we enalbe prefetching of batches to improve memory access latency during training)\n",
    "dataset = dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've prepared the training dataset, we can build our model. Instead of using a Keras Sequential model, we are going to build our own custom model class for easier access to the states from all the RNN cells which will be needed later for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(chars_to_index.get_vocabulary())\n",
    "\n",
    "# model hyperparameters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "\n",
    "        # initilize the layers\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = Dense(vocab_size) # note that we don't include a softmax activation here and instead will set from_logist=True in our loss function\n",
    "\n",
    "    def call(self, inputs, states=None, return_states=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_states:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with 2 RNN layers\n",
    "class customModel2(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn1_units, rnn2_units):\n",
    "        super().__init__(self)\n",
    "\n",
    "        # initilize the layers\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru1 = GRU(rnn1_units, return_sequences=True, return_state=True)\n",
    "        self.gru2 = GRU(rnn2_units, return_sequences=True, return_state=True)\n",
    "        self.dense1 = Dense(2*vocab_size, activation='relu')\n",
    "        self.dense2 = Dense(vocab_size) # note that we don't include a softmax activation here and instead will set from_logist=True in our loss function\n",
    "\n",
    "    def call(self, inputs, states=None, return_states=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states1 = self.gru1.get_initial_state(x)\n",
    "            states2 = self.gru2.get_initial_state(x)\n",
    "        else:\n",
    "            states1, states2 = states\n",
    "        x, states1 = self.gru1(x, initial_state=states1, training=training)\n",
    "        x, states2 = self.gru2(x, initial_state=states2, training=training)\n",
    "        x = self.dense1(x, training=training)\n",
    "        x = self.dense2(x, training=training)\n",
    "\n",
    "        if return_states:\n",
    "            return x, (states1, states2)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = customModel2(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, RNN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 21:33:02.014676: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [1115394]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-09 21:33:02.014908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [1115394]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "input: b\" I desire my life\\nOnce more to look on him.\\n\\nFLORIZEL:\\nBy his command\\nHave I here touch'd Sicilia an\"\n",
      "target: b\"I desire my life\\nOnce more to look on him.\\n\\nFLORIZEL:\\nBy his command\\nHave I here touch'd Sicilia and\"\n",
      "prediction: b\"hk3nF\\nTog[UNK]Pm\\nyRwDGWKChXeC?':FIZZGDpDBw?pTBxCHrbT!B!FYUN$AGMryXmWanINOQL:.e?ysqQ$[UNK]xNYdsz;r&W?yEpCPK!r\"\n"
     ]
    }
   ],
   "source": [
    "# now lets test the model by running an input batch through it and inspecting the output shape\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    # compute prediction\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "    # since the prediction for each character in the sequence is a probability distribution over the vocabulary, we can sample from the distribution to get a concrete predicted character. We will only do this for the first senquence in the batch\n",
    "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "    sampled_indices = tf.squeeze(sampled_indices, axis=-1)\n",
    "\n",
    "    # lets compare the prediction to the target for the first sequence in the batch \n",
    "    print(f\"input: {text_from_indices(input_example_batch[0])}\")\n",
    "    print(f\"target: {text_from_indices(target_example_batch[0])}\")\n",
    "    print(f\"prediction: {text_from_indices(sampled_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the predicted next characters are complete gibberish because the model weights are randomly initialized and haven't been trained yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model2_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    multiple                  16896     \n",
      "                                                                 \n",
      " gru_22 (GRU)                multiple                  3938304   \n",
      "                                                                 \n",
      " gru_23 (GRU)                multiple                  6297600   \n",
      "                                                                 \n",
      " dense_15 (Dense)            multiple                  135300    \n",
      "                                                                 \n",
      " dense_16 (Dense)            multiple                  8778      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,396,878\n",
      "Trainable params: 10,396,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this model can accomodate input sequences of arbitrary length and so the output shape is shown as 'multiple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss: 4.189546585083008\n",
      "exp(Mean loss): 65.99285888671875 => approximately equal to vocab size\n"
     ]
    }
   ],
   "source": [
    "# loss function (set the from_logits flag to True since the output layer computes logits, i.e. its a linear layer)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# test the loss function on the prevoiusly computed example prediction\n",
    "example_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(f\"Mean loss: {example_loss}\")\n",
    "print(f\"exp(Mean loss): {np.exp(example_loss)} => approximately equal to vocab size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss=loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for larger models, its safe practice to frequently dump checkpoints of trained weights on disk\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "# checkpoint file name\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"chkpt_{epoch}\")\n",
    "\n",
    "# callback function for dumping weights to checkpoint files\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 21:33:16.156688: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-09 21:33:16.157943: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-09 21:33:16.158852: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-09 21:33:16.228856: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-09 21:33:16.229704: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-09 21:33:16.230412: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-09 21:33:16.595741: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-09 21:33:16.596789: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-09 21:33:16.597726: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-09 21:33:16.661487: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-09 21:33:16.662197: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-09 21:33:16.662930: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 11s 49ms/step - loss: 2.4920\n",
      "Epoch 2/50\n",
      "172/172 [==============================] - 9s 47ms/step - loss: 1.7226\n",
      "Epoch 3/50\n",
      "172/172 [==============================] - 9s 47ms/step - loss: 1.4941\n",
      "Epoch 4/50\n",
      "172/172 [==============================] - 9s 47ms/step - loss: 1.3920\n",
      "Epoch 5/50\n",
      "172/172 [==============================] - 9s 47ms/step - loss: 1.3300\n",
      "Epoch 6/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 1.2807\n",
      "Epoch 7/50\n",
      "172/172 [==============================] - 9s 47ms/step - loss: 1.2407\n",
      "Epoch 8/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 1.1992\n",
      "Epoch 9/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 1.1576\n",
      "Epoch 10/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 1.1122\n",
      "Epoch 11/50\n",
      "172/172 [==============================] - 9s 47ms/step - loss: 1.0628\n",
      "Epoch 12/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 1.0043\n",
      "Epoch 13/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.9394\n",
      "Epoch 14/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.8698\n",
      "Epoch 15/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.7962\n",
      "Epoch 16/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.7209\n",
      "Epoch 17/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.6465\n",
      "Epoch 18/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.5788\n",
      "Epoch 19/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.5142\n",
      "Epoch 20/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.4583\n",
      "Epoch 21/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.4108\n",
      "Epoch 22/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.3706\n",
      "Epoch 23/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.3398\n",
      "Epoch 24/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.3112\n",
      "Epoch 25/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2912\n",
      "Epoch 26/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2743\n",
      "Epoch 27/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2644\n",
      "Epoch 28/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2529\n",
      "Epoch 29/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2459\n",
      "Epoch 30/50\n",
      "172/172 [==============================] - 10s 54ms/step - loss: 0.2408\n",
      "Epoch 31/50\n",
      "172/172 [==============================] - 10s 54ms/step - loss: 0.2339\n",
      "Epoch 32/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2270\n",
      "Epoch 33/50\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 0.2275\n",
      "Epoch 34/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2272\n",
      "Epoch 35/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2241\n",
      "Epoch 36/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2172\n",
      "Epoch 37/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2133\n",
      "Epoch 38/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2085\n",
      "Epoch 39/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2035\n",
      "Epoch 40/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2037\n",
      "Epoch 41/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2069\n",
      "Epoch 42/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2125\n",
      "Epoch 43/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2130\n",
      "Epoch 44/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2172\n",
      "Epoch 45/50\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 0.2141\n",
      "Epoch 46/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.2076\n",
      "Epoch 47/50\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 0.2011\n",
      "Epoch 48/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.1975\n",
      "Epoch 49/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.1943\n",
      "Epoch 50/50\n",
      "172/172 [==============================] - 9s 48ms/step - loss: 0.1952\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "# train the model\n",
    "history = model.fit(dataset, epochs=NUM_EPOCHS, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained model to generate text: We will now create a one-step model that computes outputs for a single forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_to_index, index_to_chars, temperature=1.0):\n",
    "        super().__init__(self)\n",
    "        self.model = model\n",
    "        self.chars_to_index = chars_to_index\n",
    "        self.index_to_chars = index_to_chars\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # create a mask to filter out any OOV tokens generated \n",
    "        # the mask is just a vector of length equal to vocabulary size with -infinity at the position corresponding to the '[UNK]' \n",
    "        # character and zeros everywhere else this vector will be added to the model output predictions so that the logit which \n",
    "        # corresponds to the [UNK] token is always -infinity and therefore always has zero probability\n",
    "        UNK_position = self.chars_to_index(['[UNK]']).numpy()[0]\n",
    "        prediction_mask = np.zeros(shape=(VOCAB_SIZE))\n",
    "        prediction_mask[UNK_position] = -float('inf')\n",
    "        prediction_mask = tf.convert_to_tensor(prediction_mask, dtype='float32')\n",
    "\n",
    "    # use tensorflow function decorator for performance boost\n",
    "    @tf.function\n",
    "    def generate_one_step(self, input_text, states=None):\n",
    "        # convert the input text to sequence of character indices\n",
    "        input_chars = tf.strings.unicode_split(input_text, input_encoding='UTF-8')\n",
    "        input_indices = self.chars_to_index(input_chars).to_tensor()\n",
    "\n",
    "        # comute prediction\n",
    "        predicted_logits, states = self.model(inputs=input_indices, states=states, return_states=True)\n",
    "\n",
    "        # since we only want the predicted next character after the input sequence, we just keep the last character from the predicted sequence\n",
    "        print(predicted_logits)\n",
    "        predicted_logits = predicted_logits[:,-1,:]      \n",
    "        predicted_logits = predicted_logits / self.temperature  # normalise by the temperture parameter\n",
    "\n",
    "        # apply mask to prevent [UNK] from being generated\n",
    "        predicted_logits = predicted_logits + prediction_mask\n",
    "\n",
    "        # now sample the predicted probability distribution \n",
    "        predicted_indices = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_indices = tf.squeeze(predicted_indices, axis=-1)\n",
    "\n",
    "        # convert the predicted indices to characters\n",
    "        predicted_chars = self.index_to_chars(predicted_indices)\n",
    "\n",
    "        # return the predicted characters and model state \n",
    "        # (the model state will be reused to generate more characters in the sequence)\n",
    "        return predicted_chars, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a one-step model with default temperature parameter value\n",
    "one_step_model = oneStep(model, chars_to_index, index_to_chars, temperature = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, numChars):\n",
    "\n",
    "    print(f\"Generating {numChars} characters from seed '{input_text}'\")\n",
    "    generated_text = input_text\n",
    "    input_text = tf.constant([input_text])\n",
    "    states = None\n",
    "\n",
    "    start_time = time.time()\n",
    "    # generate the specified number of characters\n",
    "    for i in range(numChars):\n",
    "        input_text, states = one_step_model.generate_one_step(input_text, states)\n",
    "        next_char = input_text.numpy()[0].decode(\"utf-8\")\n",
    "        generated_text += next_char\n",
    "        #print(f\"Predicted next character = {next_char}\")\n",
    "        \n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Text generation run time: {end_time-start_time} seconds\")\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 characters from seed 'ROMEO:'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 22:32:00.380105: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-09 22:32:00.384235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-09 22:32:00.385883: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-09 22:32:00.517949: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-09 22:32:00.519401: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-09 22:32:00.521068: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"custom_model2_9/dense_16/BiasAdd:0\", shape=(1, None, 66), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 22:32:00.917129: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-09 22:32:00.918961: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-09 22:32:00.920400: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-09 22:32:01.046046: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-09 22:32:01.047331: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-09 22:32:01.048383: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"custom_model2_9/dense_16/BiasAdd:0\", shape=(1, None, 66), dtype=float32)\n",
      "Text generation run time: 9.821748971939087 seconds\n",
      "\n",
      "Seed text: 'ROMEO:' \n",
      "Generated text: \n",
      "\n",
      "ROMEO:\n",
      "Whence art thou and perform'd!\n",
      "\n",
      "COMINIUS:\n",
      "You have fought together.\n",
      "\n",
      "MENENIUS:\n",
      "One word more, one word.\n",
      "This till the veriest way they are true.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Here's such adversion to despair an enemy's,\n",
      "Is very nature and the law,\n",
      "And tutor us to't.\n",
      "\n",
      "POLIXENES:\n",
      "Masters, lady! lady!\n",
      "Ale, hold, here Cominius, sir; here cut thyself\n",
      "Had been by, to resign executed, and still their powers\n",
      "Of citizens.\n",
      "\n",
      "First Citizen:\n",
      "I think 'twill serve, if any thing that I have,\n",
      "To seek the panty and the man give thee.\n",
      "\n",
      "LADY GREY:\n",
      "To do them good and bad, that cannot do't.\n",
      "\n",
      "FLORIZEL:\n",
      "Whence are you?\n",
      "Here's no place doth lends them sit so.\n",
      "\n",
      "GLOUCESTER:\n",
      "I know so. But, gentle Lady:\n",
      "Of every dost thou make done with a seas\n",
      "Than a traitor and a little while!\n",
      "You have wounds come to the sun.\n",
      "Could we but long till twice a bar and loss\n",
      "And see in storms of them accomplished:\n",
      "Such as he says your worships have married Juliet:\n",
      "Said he not have we heard his surfeit to the Duke of Norfolk:\n",
      "Ratcliff, thyself, or I'l\n"
     ]
    }
   ],
   "source": [
    "# test the text generator\n",
    "seed_text = \"ROMEO:\"\n",
    "numChars = 1000\n",
    "\n",
    "generated_text = generate_text(seed_text, numChars)\n",
    "print(f\"\\nSeed text: '{seed_text}' \\nGenerated text: \\n\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
